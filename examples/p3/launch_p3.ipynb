{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:\n",
      "AcesArguments(environement_name='p3', path_archive='', path_save='/home/flowers/work/aces/save_data/', name_experience='aces_P3_expe', n_generation=100, num_solutions=50, batch_size=32, n_fewshot_examples=3, max_descriptor_targeted=5, mode_sampling_goal='uniform', seed=0, sampling_strategy_examples_from_niche='soft_normalised', temperature_sampling_strategy_examples_from_niche=0.2, puzzle_generation_strategy='aces_elm', difficulty_min_target=90, difficulty_max_target=100, save_every_n_generations=3, path_checkpoint_archive='')\n",
      "LLMArguments(model_name_or_path='/home/flowers/work/hf/Qwen2.5-0.5B-Instruct', online=False, base_url='http://localhost:8000', api_key='', gpu=1, temperature=1.0, temperature_labeller=0.0, min_p=0.05, max_tokens=4000, max_model_length=25000, swap_space=5)\n",
      "init LLM client\n",
      "INFO 12-06 13:33:40 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 12-06 13:33:40 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/home/flowers/work/hf/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='/home/flowers/work/hf/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=25000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/flowers/work/hf/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-06 13:33:40 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-06 13:33:41 model_runner.py:1072] Starting to load model /home/flowers/work/hf/Qwen2.5-0.5B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25949f33c9b4f6fbeed6438d8459fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 13:33:41 model_runner.py:1077] Loading model weights took 0.9276 GB\n",
      "INFO 12-06 13:33:42 worker.py:232] Memory profiling results: total_gpu_memory=15.70GiB initial_memory_usage=1.26GiB peak_torch_memory=2.36GiB memory_usage_post_profile=1.29GiB non_torch_memory=0.36GiB kv_cache_size=11.41GiB gpu_memory_utilization=0.90\n",
      "INFO 12-06 13:33:42 gpu_executor.py:113] # GPU blocks: 62336, # CPU blocks: 27306\n",
      "INFO 12-06 13:33:42 gpu_executor.py:117] Maximum concurrency for 25000 tokens per request: 39.90x\n",
      "INFO 12-06 13:33:46 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-06 13:33:46 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-06 13:34:00 model_runner.py:1518] Graph capturing finished in 14 secs, took 0.15 GiB\n",
      "LLM client initialized\n",
      "load initial archive:  /home/flowers/work/aces/aces/environement/p3/preprocess_p3_emb_dedup_puzzles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  61%|██████    | 84/138 [00:18<00:00, 54.49it/s, est. speed input: 11867.55 toks/s, output: 3090.70 toks/s]"
     ]
    }
   ],
   "source": [
    "from aces.environement.p3.aces_p3 import ACES_p3\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AcesArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.DataTrainingArguments\n",
    "    \"\"\"\n",
    "\n",
    "    environement_name : str = field( default = \"p3\", metadata={\"help\": \"environment name\"})\n",
    "    path_archive : str = field(\n",
    "        default = \"\", \n",
    "        metadata={\"help\": \"path to the archive if empty load the default archive\"}\n",
    "    )\n",
    "    path_save: str = field( \n",
    "        default = \"/home/flowers/work/aces/save_data/\",\n",
    "        metadata={\"help\": \"path to save the archive\"}\n",
    "    )\n",
    "    name_experience: str = field( default = \"aces_P3_expe\", metadata={\"help\": \"name of the experience (use for saving)\"})\n",
    "    n_generation: int = field( default = 100, metadata={\"help\": \"number of generation to run\"})\n",
    "    num_solutions: int = field(\n",
    "        default = 50, metadata={\"help\": \"number of solutions to generate to compute the difficulty score\"}\n",
    "    )\n",
    "    batch_size: int = field( default = 32, metadata={\"help\": \"number of puzzles to create per generation\"})\n",
    "    n_fewshot_examples: int = field( default = 3, metadata={\"help\": \"number of example in context\" })\n",
    "    max_descriptor_targeted: int = field(\n",
    "        default = 5,\n",
    "        metadata={\"help\": \"number of max descriptor to target (at most `max_descriptor_targeted` semantic descriptor sample as goal)\"})\n",
    "    mode_sampling_goal: str = field(\n",
    "        default = \"uniform\",\n",
    "        metadata={\"help\": \"['uniform','smart','none'], uniform sample goal uniformely, smart: sample unexplored goal close that are within 1 of distance of already explored goal in the semantic space\"})\n",
    "    seed: int = field(default=0)\n",
    "    sampling_strategy_examples_from_niche: str = field(\n",
    "        default='soft_normalised',\n",
    "        metadata={\"help\": \"sampling strategy to sample examples from a niche, choice: 'uniform','prob_best_5','soft_normalised'; need to explain difference\"}\n",
    "    )\n",
    "    temperature_sampling_strategy_examples_from_niche: float = field(\n",
    "        default= 0.2, \n",
    "        metadata={\"help\": \"temperature softmax to sample example given their fitness given a niche\"}\n",
    "    )\n",
    "    puzzle_generation_strategy: str = field(\n",
    "    default= \"aces_elm\", \n",
    "    metadata={\"help\":\"startegy to generate new puzzle, choice: ['aces','aces_elm'] todo 'wizard_coder'\"})\n",
    "    difficulty_min_target: int = field(default = 90, metadata={\"help\":\"difficulty min to target /100\"})\n",
    "    difficulty_max_target: int = field(default = 100, metadata={\"help\":\"difficulty min to target /100\"})\n",
    "    save_every_n_generations: int = field(default = 3, metadata={\"help\":\"save archive every n generations\"})\n",
    "    path_checkpoint_archive: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\":\"if != '' resume experiment from the given a archive checkpoint \"})\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class LLMArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.DataTrainingArguments\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"/home/flowers/work/hf/Qwen2.5-0.5B-Instruct\",#\"/home/flowers/work/hf/Qwen2.5-Coder-3B-Instruct\",\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    online: Optional[bool] = field(\n",
    "        default = False,\n",
    "        metadata={\n",
    "            \"help\": \"use vllm server if True else use offline vllm\"\n",
    "        },\n",
    "    )\n",
    "    base_url: Optional[str] = field(\n",
    "        default=\"http://localhost:8000\",\n",
    "        metadata={\n",
    "            \"help\": \"base url for vllm server\"\n",
    "        },\n",
    "    )\n",
    "    api_key: Optional[str] = field(\n",
    "        default=\"\",\n",
    "        metadata={\n",
    "            \"help\": \"api key \"\n",
    "        },\n",
    "    )\n",
    "    gpu: Optional[int] = field(\n",
    "        default = 1,\n",
    "        metadata={\n",
    "            \"help\": \"number of gpus to use (vllm)\"\n",
    "        },\n",
    "    )\n",
    "    temperature: Optional[float] = field(\n",
    "        default = 1.0,\n",
    "        metadata={\n",
    "            \"help\": \"temperature\"\n",
    "        },\n",
    "    )\n",
    "    temperature_labeller: Optional[float] = field(\n",
    "        default = 0.,\n",
    "        metadata={\n",
    "            \"help\": \"temperature labeller (semantic descriptor)\"\n",
    "        },\n",
    "    )\n",
    "    min_p: Optional[float] = field(\n",
    "        default = 0.05,\n",
    "        metadata={\n",
    "            \"help\": \"min_p\"\n",
    "        },\n",
    "    )\n",
    "    max_tokens: Optional[int] = field(\n",
    "        default = 4000,\n",
    "        metadata={\n",
    "            \"help\": \"max tokens\"\n",
    "        },\n",
    "    )\n",
    "    max_model_length: Optional[int] = field(\n",
    "        default = 25000,\n",
    "        metadata={\n",
    "            \"help\": \"max context size\"\n",
    "        },\n",
    "    )\n",
    "    swap_space: Optional[float] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"swap space (RAM memory for cache)\"\n",
    "        }\n",
    "    )\n",
    "parser = HfArgumentParser((AcesArguments,LLMArguments))\n",
    "# aces_args, llm_args = parser.parse_args_into_dataclasses()\n",
    "aces_args, llm_args = AcesArguments(), LLMArguments()\n",
    "print(\"args:\")\n",
    "print(aces_args)\n",
    "print(llm_args)\n",
    "aces= ACES_p3(aces_args, llm_args)\n",
    "aces.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
